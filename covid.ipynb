{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from   datetime import datetime, timedelta\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is more dataset than these two on github\n",
    "global recovered cases, US by county.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONFIRMED_DATASET = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "DEATHS_DATASET = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LOCAL_CFILE = 'time_series_covid19_confirmed_global.csv'\n",
    "LOCAL_DFILE = 'time_series_covid19_deaths_global.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download():\n",
    "        # use urllib2 instead ?\n",
    "        pd.read_csv( CONFIRMED_DATASET ).to_csv( LOCAL_CFILE )\n",
    "        pd.read_csv( DEATHS_DATASET    ).to_csv( LOCAL_DFILE )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refresh():\n",
    "    '''downloads files from github if too old'''\n",
    "    \n",
    "    now = datetime.now()\n",
    "    \n",
    "    if  os.path.isfile( LOCAL_CFILE ) is False or \\\n",
    "        os.path.isfile( LOCAL_DFILE ) is False or \\\n",
    "        ( now - datetime.fromtimestamp( os.path.getmtime( LOCAL_CFILE) ) ) > timedelta( hours=2 ) or \\\n",
    "        ( now - datetime.fromtimestamp( os.path.getmtime( LOCAL_DFILE) ) ) > timedelta( hours=2 ) :\n",
    "                \n",
    "                download()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    '''load csv files from disk into in memory pandas DataFrame'''\n",
    "    \n",
    "    refresh()\n",
    "    # load from memory AND remove full columns with NaN\n",
    "    df_confirmed = pd.read_csv( LOCAL_CFILE ).dropna( axis=1, how='all', inplace=False )\n",
    "    df_deaths    = pd.read_csv( LOCAL_DFILE ).dropna( axis=1, how='all', inplace=False )\n",
    "    \n",
    "    return (df_confirmed, df_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_confirmed, df_deaths = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_of_interest =  [ \"France\", \"Italy\", \"United Kingdom\", \"US\", \"China\" ]\n",
    "european_countries = [\n",
    "    'Albania',\n",
    "    'Austria',\n",
    "    'Belarus',\n",
    "    'Belgium',\n",
    "    'Bosnia',\n",
    "    'Bulgaria',\n",
    "    'Croatia',\n",
    "    'Cyprus',\n",
    "    'Czech Republic',\n",
    "    'Denmark',\n",
    "    'Estonia',\n",
    "    'Finland',\n",
    "    'France',\n",
    "    'Germany',\n",
    "    'Greece',\n",
    "    'Hungary',\n",
    "    'Latvia',\n",
    "    'Ireland',\n",
    "    'Italy',\n",
    "    'Lithuania',\n",
    "    'Luxembourg',\n",
    "    'Malta',\n",
    "    'Netherlands',\n",
    "    'Norway',\n",
    "    'Portugal',\n",
    "    'Poland',\n",
    "    'Romania',\n",
    "    'San Marino',\n",
    "    'Serbia',\n",
    "    'Slovakia',\n",
    "    'Slovenia',\n",
    "    'Spain',\n",
    "    'Sweden',\n",
    "    'Switzerland',\n",
    "    'United Kingdom'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the dataset\n",
    "\n",
    "three plots\n",
    "1. total cases against date\n",
    "2. new cases ( smoothed over a rolling window of 7 days ) against date\n",
    "3. log/log plot of smoothed new cases as a function of total cases\n",
    "\n",
    "## About that last plot\n",
    "\n",
    "lemma:\n",
    "f is a function f(x) : R->R such that f'(x) = k f(x)  **if and only if** there exist c,k in R such that f(x) = c exp ( k x )\n",
    "\n",
    "this is one of the well known characterization of the exponential function.\n",
    "\n",
    "As a result if we consider f(t) = total cases on day t, f'(t) can be approximated with the finite difference: new cases per day.\n",
    "\n",
    "since there's a variability we smoothe the differnce over a window ...\n",
    "in the limit, that's f'(t) ...\n",
    "\n",
    "so, in that plot, the time is not explicitly apparent, and a linear trend means exponential behavior in time.\n",
    "Without having to shift the curves horizontally to a common reference frame.\n",
    "\n",
    "The last plot obviously does not need to have log axis to show deviation from exponential behavior.\n",
    "Log/Log plot makes it easier to handle the beginning of the curve, and makes the end of it more dramatic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a property of the data format\n",
    "non_date_columns = 5\n",
    "\n",
    "# these are choices for display\n",
    "days_to_ignore = 30\n",
    "\n",
    "window_size = 7\n",
    "\n",
    "figsize=(30,90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, (ax1, ax2, ax3) = plt.subplots( 3, 1, figsize=figsize )\n",
    "\n",
    "for country in countries_of_interest:\n",
    "    # pick the right row, and the time series portion only. shouldn't have to sum over regions anymore.\n",
    "    # that's a remnant of the previous dataeset format\n",
    "    total_cases = df_confirmed.loc[df_confirmed['Country/Region'] == country].filter( regex='.*/20', axis=1 ).sum(0)\n",
    "    # \n",
    "    new_cases = total_cases.diff().dropna()\n",
    "    smoothed_new_cases = new_cases.rolling( window_size, win_type='boxcar' ).sum().dropna() / window_size\n",
    "    \n",
    "    ax1.plot( total_cases[(non_date_columns + days_to_ignore):], lw=3, alpha=0.5, label=country )\n",
    "    \n",
    "    ax2.plot( smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "              lw=3,\n",
    "              alpha=0.5,\n",
    "              label=country )\n",
    "    \n",
    "    # read above to understand this plot.\n",
    "    ax3.plot( total_cases[(non_date_columns + days_to_ignore + 1 + window_size -1):],\n",
    "              smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "              lw=3,\n",
    "              alpha=0.5,\n",
    "              label=country )\n",
    "             \n",
    "# european countries as a sum\n",
    "european_total_cases = df_confirmed.loc[df_confirmed['Country/Region'].isin(european_countries)].filter( regex='.*/20', axis=1 ).sum(0)\n",
    "european_new_cases   = european_total_cases.diff().dropna()\n",
    "european_smoothed_new_cases = european_new_cases.rolling( window_size, win_type='boxcar' ).sum().dropna() / window_size\n",
    "\n",
    "ax1.plot( european_total_cases[(non_date_columns + days_to_ignore):], lw=3, alpha=0.5, label='Europe' )\n",
    "ax2.plot( european_smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "          lw=3,\n",
    "          alpha=0.5,\n",
    "          label='Europe' )\n",
    "ax3.plot( european_total_cases[(non_date_columns + days_to_ignore + 1 + window_size -1):],\n",
    "          european_smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "          lw=3,\n",
    "          alpha=0.5,\n",
    "          label='Europe' )\n",
    "\n",
    "\n",
    "# China as a sum over regions\n",
    "chinese_total_cases = df_confirmed.loc[df_confirmed['Country/Region'].isin(['China'])].filter( regex='.*/20', axis=1 ).sum(0)\n",
    "chinese_new_cases   = chinese_total_cases.diff().dropna()\n",
    "chinese_smoothed_new_cases = chinese_new_cases.rolling( window_size, win_type='boxcar' ).sum().dropna() / window_size\n",
    "\n",
    "ax1.plot( chinese_total_cases[(non_date_columns + days_to_ignore):], lw=3, alpha=0.5, label='China' )\n",
    "ax2.plot( chinese_smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "          lw=3,\n",
    "          alpha=0.5,\n",
    "          label='China' )\n",
    "# all of China's data\n",
    "# the plot is not in time. we want all the data ...\n",
    "ax3.plot( chinese_total_cases[(non_date_columns  + 1 + window_size -1):],\n",
    "          chinese_smoothed_new_cases[( non_date_columns ):],\n",
    "          lw=3,\n",
    "          alpha=0.5,\n",
    "          label='China' )\n",
    "\n",
    "\n",
    "ax1.set_title('Confirmed cases of Covid as a function of date')\n",
    "ax2.set_title('New cases of Covid as a function of date (smoothed over 7 days)')\n",
    "ax3.set_title( 'log( smoothed new cases) as a function of total cases. linear trend means exponential growth over time')\n",
    "\n",
    "fontsize=20 \n",
    "\n",
    "for ax in [ax1, ax2, ax3]:\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "        ax.legend()\n",
    "\n",
    "        for item in ([ ax.title, ax.xaxis.label, ax.yaxis.label ]\n",
    "                     + ax.get_xticklabels()\n",
    "                     + ax.get_yticklabels()\n",
    "                     + ax.get_legend().get_texts()  ):\n",
    "                item.set_fontsize(fontsize)\n",
    "\n",
    "        plt.setp( ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "# the last plot does not have to be log/log\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_list( df_confirmed, df_label, european_countries, label, column ):\n",
    "    '''Plot for an arbitrarly list of countries'''\n",
    "    # this is a property of the data format\n",
    "    non_date_columns = 5\n",
    "\n",
    "    # these are choices for display\n",
    "    days_to_ignore = 30\n",
    "\n",
    "    window_size = 7\n",
    "\n",
    "    figsize=(30,90)\n",
    "\n",
    "    _, (ax1, ax2, ax3) = plt.subplots( 3, 1, figsize=figsize )\n",
    "\n",
    "    for country in european_countries:\n",
    "        # pick the right row, and the time series portion only. shouldn't have to sum over regions anymore.\n",
    "        # that's a remnant of the previous dataeset format\n",
    "        total_cases = df_confirmed.loc[df_confirmed[column] == country].filter( regex='.*/20', axis=1 ).sum(0)\n",
    "        new_cases = total_cases.diff().dropna()\n",
    "        smoothed_new_cases = new_cases.rolling( window_size, win_type='boxcar' ).sum().dropna() / window_size\n",
    "\n",
    "        ax1.plot( total_cases[(non_date_columns + days_to_ignore):], lw=3, alpha=0.5, label=country )\n",
    "\n",
    "        ax2.plot( smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "                  lw=3,\n",
    "                  alpha=0.5,\n",
    "                  label=country )\n",
    "\n",
    "        ax3.plot( total_cases[(non_date_columns + days_to_ignore + 1 + window_size -1):],\n",
    "                  smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "                  lw=3,\n",
    "                  alpha=0.5,\n",
    "                  label=country )\n",
    "\n",
    "    # european countries as a sum\n",
    "    european_total_cases = df_confirmed.loc[df_confirmed[column].isin(european_countries)].filter( regex='.*/20', axis=1 ).sum(0)\n",
    "    european_new_cases   = european_total_cases.diff().dropna()\n",
    "    european_smoothed_new_cases = european_new_cases.rolling( window_size, win_type='boxcar' ).sum().dropna() / window_size\n",
    "\n",
    "    ax1.plot( european_total_cases[(non_date_columns + days_to_ignore):], lw=3, alpha=0.5, label='All Europe' )\n",
    "    ax2.plot( european_smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "              lw=3,\n",
    "              alpha=0.5,\n",
    "              label='All ' + label )\n",
    "    ax3.plot( european_total_cases[(non_date_columns + days_to_ignore + 1 + window_size -1):],\n",
    "              european_smoothed_new_cases[( non_date_columns + days_to_ignore):],\n",
    "              lw=3,\n",
    "              alpha=0.5,\n",
    "              label='All' + label )\n",
    "\n",
    "\n",
    "    ax1.set_title( df_label + ' of Covid as a function of date')\n",
    "    ax2.set_title('new ' + df_label + ' of Covid as a function of date (smoothed over 7 days)')\n",
    "    ax3.set_title( 'log( smoothed new ' + df_label + ') as a function of total ' + df_label + '. linear trend means exponential growth over time')\n",
    "\n",
    "    fontsize=20 \n",
    "\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "            ax.xaxis.set_major_locator(plt.MaxNLocator(12))\n",
    "            ax.legend()\n",
    "\n",
    "            for item in ([ ax.title, ax.xaxis.label, ax.yaxis.label ]\n",
    "                         + ax.get_xticklabels()\n",
    "                         + ax.get_yticklabels()\n",
    "                         + ax.get_legend().get_texts()  ):\n",
    "                    item.set_fontsize(fontsize)\n",
    "\n",
    "            plt.setp( ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "    # the last plot does not have to be log log\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_yscale('log')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Europe only plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_list( df_confirmed, 'confirmed cases', european_countries, 'Europe', 'Country/Region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US state by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "US_CONFIRMED_DATASET = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "US_DEATHS_DATASET = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
    "\n",
    "LOCAL_US_CFILE = 'time_series_covid19_confirmed_US.csv'\n",
    "LOCAL_US_DFILE = 'time_series_covid19_deaths_US.csv'\n",
    "\n",
    "def download_US():\n",
    "        # use urllib2 instead ?\n",
    "        pd.read_csv( US_CONFIRMED_DATASET ).to_csv( LOCAL_US_CFILE )\n",
    "        pd.read_csv( US_DEATHS_DATASET    ).to_csv( LOCAL_US_DFILE )\n",
    "        \n",
    "def refresh_US():\n",
    "    '''downloads files from github if too old'''\n",
    "    \n",
    "    now = datetime.now()\n",
    "    \n",
    "    if  os.path.isfile( LOCAL_US_CFILE ) is False or \\\n",
    "        os.path.isfile( LOCAL_US_DFILE ) is False or \\\n",
    "        ( now - datetime.fromtimestamp( os.path.getmtime( LOCAL_US_CFILE) ) ) > timedelta( hours=2 ) or \\\n",
    "        ( now - datetime.fromtimestamp( os.path.getmtime( LOCAL_US_DFILE) ) ) > timedelta( hours=2 ) :\n",
    "                \n",
    "                download_US\n",
    "\n",
    "def load_US():\n",
    "    '''load csv files from disk into in memory pandas DataFrame'''\n",
    "    \n",
    "    refresh_US()\n",
    "    # load from memory AND remove full columns with NaN\n",
    "    df_us_confirmed = pd.read_csv( LOCAL_US_CFILE ).dropna( axis=1, how='all', inplace=False )\n",
    "    df_us_deaths    = pd.read_csv( LOCAL_US_DFILE ).dropna( axis=1, how='all', inplace=False )\n",
    "    \n",
    "    return (df_us_confirmed, df_us_deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_us_confirmed, df_us_deaths = load_US()\n",
    "\n",
    "#print df_us_confirmed\n",
    "\n",
    "US_states = [ \n",
    "    'California', \n",
    "    'Florida', \n",
    "    'Illinois',\n",
    "    'Mississipi',\n",
    "    'New Jersey', \n",
    "    'New York',\n",
    "    'Ohio',\n",
    "    'Oregon', \n",
    "    'Texas',\n",
    "    'Utah',\n",
    "    'Washington' \n",
    "]\n",
    "\n",
    "# I make sum in there, I could have matched Country_Region to US to plot all...\n",
    "plot_list( df_us_confirmed, 'confirmed cases', US_states, 'US subset', 'Province_State')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# California counties of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
